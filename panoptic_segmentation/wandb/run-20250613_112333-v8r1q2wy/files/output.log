Frequency file exists. Using weights from frequency file
Using no loss weighting...
[32m[06/13 11:23:34 d2.engine.defaults]: [0mModel:
CTMinVIS(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): VideoMultiScaleMaskedTransformerDecoder_dvisPlus(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=125, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (reid_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion VideoSetCriterion
      matcher: Matcher VideoHungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 124
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (image_matcher): Matcher HungarianMatcher
      cost_class: 2.0
      cost_mask: 5.0
      cost_dice: 5.0
  (cl_plugin): CTCLPlugin()
)
[32m[06/13 11:23:34 fvcore.common.checkpoint]: [0m[Checkpointer] Loading from checkpoints/ctvis_r50_vspw.pth ...
Loading samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:01<00:00, 33.01it/s]
[32m[06/13 11:23:36 d2.data.common]: [0mSerializing 72 elements to byte tensors and concatenating them all ...
[32m[06/13 11:23:36 d2.data.common]: [0mSerialized dataset takes 14.05 MiB
COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/13 11:23:36 d2.evaluation.evaluator]: [0mStart inference on 72 batches
/home/connecteve/miniconda3/envs/mm-or/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
43
34
51
97
91
[32m[06/13 11:23:58 d2.evaluation.evaluator]: [0mInference done 1/72. Dataloading: 0.0106 s/iter. Inference: 15.4725 s/iter. Eval: 6.1436 s/iter. Total: 21.6273 s/iter. ETA=0:25:35
43
51
91
34
47
[32m[06/13 11:24:17 d2.evaluation.evaluator]: [0mInference done 2/72. Dataloading: 0.0604 s/iter. Inference: 14.0468 s/iter. Eval: 6.0476 s/iter. Total: 20.1554 s/iter. ETA=0:23:30
43
34
51
91
[32m[06/13 11:24:35 d2.evaluation.evaluator]: [0mInference done 3/72. Dataloading: 0.0803 s/iter. Inference: 13.5077 s/iter. Eval: 6.0479 s/iter. Total: 19.6364 s/iter. ETA=0:22:34
43
34
51
[32m[06/13 11:24:53 d2.evaluation.evaluator]: [0mInference done 4/72. Dataloading: 0.0903 s/iter. Inference: 13.2196 s/iter. Eval: 5.9988 s/iter. Total: 19.3092 s/iter. ETA=0:21:53
43
34
51
[32m[06/13 11:25:12 d2.evaluation.evaluator]: [0mInference done 5/72. Dataloading: 0.0871 s/iter. Inference: 13.0624 s/iter. Eval: 5.9880 s/iter. Total: 19.1380 s/iter. ETA=0:21:22
43
34
51
31
37
37
36
36
36
47
80
28
34
43
47
25
91
97
28
28
[32m[06/13 11:25:31 d2.evaluation.evaluator]: [0mInference done 7/72. Dataloading: 0.0450 s/iter. Inference: 6.4598 s/iter. Eval: 3.0034 s/iter. Total: 9.5082 s/iter. ETA=0:10:18
43
34
28
28
[32m[06/13 11:25:49 d2.evaluation.evaluator]: [0mInference done 8/72. Dataloading: 0.0853 s/iter. Inference: 8.3544 s/iter. Eval: 3.8603 s/iter. Total: 12.3002 s/iter. ETA=0:13:07
34
43
[32m[06/13 11:26:07 d2.evaluation.evaluator]: [0mInference done 9/72. Dataloading: 0.0916 s/iter. Inference: 9.3163 s/iter. Eval: 4.2871 s/iter. Total: 13.6952 s/iter. ETA=0:14:22
43
34
47
28
[32m[06/13 11:26:25 d2.evaluation.evaluator]: [0mInference done 10/72. Dataloading: 0.0951 s/iter. Inference: 9.8956 s/iter. Eval: 4.5576 s/iter. Total: 14.5485 s/iter. ETA=0:15:02
34
43
28
28
[32m[06/13 11:26:43 d2.evaluation.evaluator]: [0mInference done 11/72. Dataloading: 0.0939 s/iter. Inference: 10.2996 s/iter. Eval: 4.7401 s/iter. Total: 15.1339 s/iter. ETA=0:15:23
43
34
37
87
72
32
47
41
28
43
51
34
85
37
25
72
91
[32m[06/13 11:27:02 d2.evaluation.evaluator]: [0mInference done 13/72. Dataloading: 0.0834 s/iter. Inference: 9.3495 s/iter. Eval: 4.2945 s/iter. Total: 13.7276 s/iter. ETA=0:13:29
43
34
31
[32m[06/13 11:27:20 d2.evaluation.evaluator]: [0mInference done 14/72. Dataloading: 0.0847 s/iter. Inference: 9.6901 s/iter. Eval: 4.4463 s/iter. Total: 14.2214 s/iter. ETA=0:13:44
43
34
25
51
[32m[06/13 11:27:38 d2.evaluation.evaluator]: [0mInference done 15/72. Dataloading: 0.0880 s/iter. Inference: 9.9568 s/iter. Eval: 4.5701 s/iter. Total: 14.6151 s/iter. ETA=0:13:53
34
43
51
[32m[06/13 11:27:56 d2.evaluation.evaluator]: [0mInference done 16/72. Dataloading: 0.0884 s/iter. Inference: 10.1717 s/iter. Eval: 4.6730 s/iter. Total: 14.9334 s/iter. ETA=0:13:56
43
91
34
51
25
[32m[06/13 11:28:14 d2.evaluation.evaluator]: [0mInference done 17/72. Dataloading: 0.0898 s/iter. Inference: 10.3534 s/iter. Eval: 4.7587 s/iter. Total: 15.2021 s/iter. ETA=0:13:56
37
43
34
51
25
36
31
97
31
25
87
25
91
43
34
92
[32m[06/13 11:28:33 d2.evaluation.evaluator]: [0mInference done 19/72. Dataloading: 0.0827 s/iter. Inference: 9.8110 s/iter. Eval: 4.5010 s/iter. Total: 14.3949 s/iter. ETA=0:12:42
43
34
28
51
36
[32m[06/13 11:28:51 d2.evaluation.evaluator]: [0mInference done 20/72. Dataloading: 0.0836 s/iter. Inference: 9.9618 s/iter. Eval: 4.5786 s/iter. Total: 14.6243 s/iter. ETA=0:12:40
43
34
55
25
97
28
[32m[06/13 11:29:09 d2.evaluation.evaluator]: [0mInference done 21/72. Dataloading: 0.0866 s/iter. Inference: 10.0885 s/iter. Eval: 4.6531 s/iter. Total: 14.8285 s/iter. ETA=0:12:36
43
34
36
97
28
28
28
28
28
[32m[06/13 11:29:27 d2.evaluation.evaluator]: [0mInference done 22/72. Dataloading: 0.0892 s/iter. Inference: 10.2160 s/iter. Eval: 4.7223 s/iter. Total: 15.0279 s/iter. ETA=0:12:31
43
34
51
97
84
36
28
[32m[06/13 11:29:45 d2.evaluation.evaluator]: [0mInference done 23/72. Dataloading: 0.0919 s/iter. Inference: 10.3277 s/iter. Eval: 4.7740 s/iter. Total: 15.1938 s/iter. ETA=0:12:24
43
34
84
36
97
28
36
[32m[06/13 11:30:04 d2.evaluation.evaluator]: [0mInference done 24/72. Dataloading: 0.0953 s/iter. Inference: 10.4295 s/iter. Eval: 4.8237 s/iter. Total: 15.3489 s/iter. ETA=0:12:16
43
34
25
84
51
36
36
[32m[06/13 11:30:22 d2.evaluation.evaluator]: [0mInference done 25/72. Dataloading: 0.0988 s/iter. Inference: 10.5182 s/iter. Eval: 4.8624 s/iter. Total: 15.4798 s/iter. ETA=0:12:07
43
34
47
84
72
51
[32m[06/13 11:30:39 d2.evaluation.evaluator]: [0mInference done 26/72. Dataloading: 0.1009 s/iter. Inference: 10.5777 s/iter. Eval: 4.8949 s/iter. Total: 15.5737 s/iter. ETA=0:11:56
43
34
51
[32m[06/13 11:30:58 d2.evaluation.evaluator]: [0mInference done 27/72. Dataloading: 0.1018 s/iter. Inference: 10.6689 s/iter. Eval: 4.9399 s/iter. Total: 15.7109 s/iter. ETA=0:11:46
43
34
47
52
28
[32m[06/13 11:31:12 d2.evaluation.evaluator]: [0mInference done 28/72. Dataloading: 0.1020 s/iter. Inference: 10.8113 s/iter. Eval: 4.7595 s/iter. Total: 15.6731 s/iter. ETA=0:11:29
43
34
28
28
[32m[06/13 11:31:32 d2.evaluation.evaluator]: [0mInference done 29/72. Dataloading: 0.0987 s/iter. Inference: 10.9342 s/iter. Eval: 4.8153 s/iter. Total: 15.8486 s/iter. ETA=0:11:21
43
34
36
52
25
37
47
25
[32m[06/13 11:31:51 d2.evaluation.evaluator]: [0mInference done 30/72. Dataloading: 0.0991 s/iter. Inference: 11.0094 s/iter. Eval: 4.8566 s/iter. Total: 15.9653 s/iter. ETA=0:11:10
43
51
34
[32m[06/13 11:32:02 d2.evaluation.evaluator]: [0mInference done 31/72. Dataloading: 0.0999 s/iter. Inference: 10.8812 s/iter. Eval: 4.8018 s/iter. Total: 15.7832 s/iter. ETA=0:10:47
43
34
25
55
36
28
[32m[06/13 11:32:21 d2.evaluation.evaluator]: [0mInference done 32/72. Dataloading: 0.0992 s/iter. Inference: 10.9613 s/iter. Eval: 4.8392 s/iter. Total: 15.9000 s/iter. ETA=0:10:35
43
34
25
25
25
28
47
97
[32m[06/13 11:32:40 d2.evaluation.evaluator]: [0mInference done 33/72. Dataloading: 0.0991 s/iter. Inference: 11.0286 s/iter. Eval: 4.8787 s/iter. Total: 16.0068 s/iter. ETA=0:10:24
43
34
36
25
91
25
97
25
[32m[06/13 11:33:00 d2.evaluation.evaluator]: [0mInference done 34/72. Dataloading: 0.1059 s/iter. Inference: 11.0880 s/iter. Eval: 4.9398 s/iter. Total: 16.1341 s/iter. ETA=0:10:13
43
34
25
25
97
[32m[06/13 11:33:19 d2.evaluation.evaluator]: [0mInference done 35/72. Dataloading: 0.1072 s/iter. Inference: 11.1509 s/iter. Eval: 4.9812 s/iter. Total: 16.2396 s/iter. ETA=0:10:00
43
34
25
25
28
36
28
[32m[06/13 11:33:38 d2.evaluation.evaluator]: [0mInference done 36/72. Dataloading: 0.1083 s/iter. Inference: 11.2055 s/iter. Eval: 5.0180 s/iter. Total: 16.3321 s/iter. ETA=0:09:47
43
34
36
28
28
25
25
25
25
25
[32m[06/13 11:33:57 d2.evaluation.evaluator]: [0mInference done 37/72. Dataloading: 0.1090 s/iter. Inference: 11.2591 s/iter. Eval: 5.0542 s/iter. Total: 16.4226 s/iter. ETA=0:09:34
43
34
25
25
51
36
[32m[06/13 11:34:17 d2.evaluation.evaluator]: [0mInference done 38/72. Dataloading: 0.1095 s/iter. Inference: 11.3174 s/iter. Eval: 5.0868 s/iter. Total: 16.5141 s/iter. ETA=0:09:21
43
34
36
25
25
97
25
[32m[06/13 11:34:36 d2.evaluation.evaluator]: [0mInference done 39/72. Dataloading: 0.1099 s/iter. Inference: 11.3713 s/iter. Eval: 5.1149 s/iter. Total: 16.5965 s/iter. ETA=0:09:07
43
34
25
97
36
36
25
[32m[06/13 11:34:55 d2.evaluation.evaluator]: [0mInference done 40/72. Dataloading: 0.1115 s/iter. Inference: 11.4142 s/iter. Eval: 5.1457 s/iter. Total: 16.6717 s/iter. ETA=0:08:53
43
34
36
25
97
25
89
[32m[06/13 11:35:15 d2.evaluation.evaluator]: [0mInference done 41/72. Dataloading: 0.1123 s/iter. Inference: 11.4742 s/iter. Eval: 5.1664 s/iter. Total: 16.7533 s/iter. ETA=0:08:39
43
34
28
89
47
28
36
92
25
25
[32m[06/13 11:35:30 d2.evaluation.evaluator]: [0mInference done 42/72. Dataloading: 0.1121 s/iter. Inference: 11.4373 s/iter. Eval: 5.1534 s/iter. Total: 16.7031 s/iter. ETA=0:08:21
43
34
36
52
28
28
47
28
97
28
[32m[06/13 11:35:49 d2.evaluation.evaluator]: [0mInference done 43/72. Dataloading: 0.1109 s/iter. Inference: 11.4791 s/iter. Eval: 5.1722 s/iter. Total: 16.7625 s/iter. ETA=0:08:06
34
43
28
28
28
28
36
[32m[06/13 11:36:08 d2.evaluation.evaluator]: [0mInference done 44/72. Dataloading: 0.1101 s/iter. Inference: 11.5178 s/iter. Eval: 5.1914 s/iter. Total: 16.8196 s/iter. ETA=0:07:50
43
34
47
47
25
25
28
28
[32m[06/13 11:36:27 d2.evaluation.evaluator]: [0mInference done 45/72. Dataloading: 0.1085 s/iter. Inference: 11.5508 s/iter. Eval: 5.2127 s/iter. Total: 16.8724 s/iter. ETA=0:07:35
43
34
91
36
48
[32m[06/13 11:36:36 d2.evaluation.evaluator]: [0mInference done 46/72. Dataloading: 0.1064 s/iter. Inference: 11.4190 s/iter. Eval: 5.1554 s/iter. Total: 16.6812 s/iter. ETA=0:07:13
43
34
28
37
[32m[06/13 11:36:55 d2.evaluation.evaluator]: [0mInference done 47/72. Dataloading: 0.1060 s/iter. Inference: 11.4575 s/iter. Eval: 5.1783 s/iter. Total: 16.7422 s/iter. ETA=0:06:58
43
34
47
28
25
[32m[06/13 11:37:15 d2.evaluation.evaluator]: [0mInference done 48/72. Dataloading: 0.1062 s/iter. Inference: 11.5006 s/iter. Eval: 5.2096 s/iter. Total: 16.8167 s/iter. ETA=0:06:43
34
43
55
47
[32m[06/13 11:37:35 d2.evaluation.evaluator]: [0mInference done 49/72. Dataloading: 0.1054 s/iter. Inference: 11.5623 s/iter. Eval: 5.2204 s/iter. Total: 16.8886 s/iter. ETA=0:06:28
43
34
91
28
37
28
51
[32m[06/13 11:37:45 d2.evaluation.evaluator]: [0mInference done 50/72. Dataloading: 0.1043 s/iter. Inference: 11.4503 s/iter. Eval: 5.1710 s/iter. Total: 16.7259 s/iter. ETA=0:06:07
43
34
25
91
28
36
[32m[06/13 11:38:03 d2.evaluation.evaluator]: [0mInference done 51/72. Dataloading: 0.1035 s/iter. Inference: 11.4813 s/iter. Eval: 5.1798 s/iter. Total: 16.7649 s/iter. ETA=0:05:52
43
34
25
25
36
47
55
55
25
97
[32m[06/13 11:38:21 d2.evaluation.evaluator]: [0mInference done 52/72. Dataloading: 0.1028 s/iter. Inference: 11.5006 s/iter. Eval: 5.1874 s/iter. Total: 16.7912 s/iter. ETA=0:05:35
43
34
36
84
37
36
25
47
[32m[06/13 11:38:39 d2.evaluation.evaluator]: [0mInference done 53/72. Dataloading: 0.1016 s/iter. Inference: 11.5236 s/iter. Eval: 5.1962 s/iter. Total: 16.8218 s/iter. ETA=0:05:19
43
34
36
25
47
37
55
25
97
[32m[06/13 11:38:48 d2.evaluation.evaluator]: [0mInference done 54/72. Dataloading: 0.1003 s/iter. Inference: 11.4161 s/iter. Eval: 5.1466 s/iter. Total: 16.6633 s/iter. ETA=0:04:59
43
34
36
97
55
36
37
[32m[06/13 11:39:07 d2.evaluation.evaluator]: [0mInference done 55/72. Dataloading: 0.0992 s/iter. Inference: 11.4398 s/iter. Eval: 5.1612 s/iter. Total: 16.7005 s/iter. ETA=0:04:43
43
34
47
47
36
28
47
28
28
55
28
25
47
[32m[06/13 11:39:25 d2.evaluation.evaluator]: [0mInference done 56/72. Dataloading: 0.0984 s/iter. Inference: 11.4638 s/iter. Eval: 5.1676 s/iter. Total: 16.7302 s/iter. ETA=0:04:27
43
34
36
47
47
25
28
[32m[06/13 11:39:44 d2.evaluation.evaluator]: [0mInference done 57/72. Dataloading: 0.0974 s/iter. Inference: 11.4912 s/iter. Eval: 5.1808 s/iter. Total: 16.7698 s/iter. ETA=0:04:11
43
34
52
28
28
72
47
[32m[06/13 11:40:02 d2.evaluation.evaluator]: [0mInference done 58/72. Dataloading: 0.0981 s/iter. Inference: 11.5084 s/iter. Eval: 5.1939 s/iter. Total: 16.8008 s/iter. ETA=0:03:55
43
34
47
36
28
47
47
28
36
28
[32m[06/13 11:40:20 d2.evaluation.evaluator]: [0mInference done 59/72. Dataloading: 0.0983 s/iter. Inference: 11.5229 s/iter. Eval: 5.2026 s/iter. Total: 16.8242 s/iter. ETA=0:03:38
43
34
37
52
25
51
51
25
76
[32m[06/13 11:40:38 d2.evaluation.evaluator]: [0mInference done 60/72. Dataloading: 0.0984 s/iter. Inference: 11.5350 s/iter. Eval: 5.2054 s/iter. Total: 16.8392 s/iter. ETA=0:03:22
43
34
28
89
91
47
[32m[06/13 11:40:57 d2.evaluation.evaluator]: [0mInference done 61/72. Dataloading: 0.0982 s/iter. Inference: 11.5569 s/iter. Eval: 5.2192 s/iter. Total: 16.8745 s/iter. ETA=0:03:05
43
34
36
89
28
[32m[06/13 11:41:16 d2.evaluation.evaluator]: [0mInference done 62/72. Dataloading: 0.0992 s/iter. Inference: 11.5793 s/iter. Eval: 5.2269 s/iter. Total: 16.9057 s/iter. ETA=0:02:49
34
43
91
84
47
[32m[06/13 11:41:34 d2.evaluation.evaluator]: [0mInference done 63/72. Dataloading: 0.1000 s/iter. Inference: 11.5935 s/iter. Eval: 5.2322 s/iter. Total: 16.9260 s/iter. ETA=0:02:32
34
43
37
28
28
28
[32m[06/13 11:41:52 d2.evaluation.evaluator]: [0mInference done 64/72. Dataloading: 0.0997 s/iter. Inference: 11.6073 s/iter. Eval: 5.2399 s/iter. Total: 16.9472 s/iter. ETA=0:02:15
34
43
[32m[06/13 11:42:10 d2.evaluation.evaluator]: [0mInference done 65/72. Dataloading: 0.0994 s/iter. Inference: 11.6192 s/iter. Eval: 5.2445 s/iter. Total: 16.9636 s/iter. ETA=0:01:58
43
34
51
47
25
91
28
37
37
[32m[06/13 11:42:21 d2.evaluation.evaluator]: [0mInference done 66/72. Dataloading: 0.0984 s/iter. Inference: 11.5518 s/iter. Eval: 5.2135 s/iter. Total: 16.8640 s/iter. ETA=0:01:41
43
34
36
47
37
28
[32m[06/13 11:42:39 d2.evaluation.evaluator]: [0mInference done 67/72. Dataloading: 0.0985 s/iter. Inference: 11.5748 s/iter. Eval: 5.2206 s/iter. Total: 16.8942 s/iter. ETA=0:01:24
43
34
84
51
49
[32m[06/13 11:42:58 d2.evaluation.evaluator]: [0mInference done 68/72. Dataloading: 0.0980 s/iter. Inference: 11.5956 s/iter. Eval: 5.2260 s/iter. Total: 16.9200 s/iter. ETA=0:01:07
43
34
36
84
[32m[06/13 11:43:16 d2.evaluation.evaluator]: [0mInference done 69/72. Dataloading: 0.0986 s/iter. Inference: 11.6079 s/iter. Eval: 5.2369 s/iter. Total: 16.9438 s/iter. ETA=0:00:50
43
34
36
25
25
49
[32m[06/13 11:43:34 d2.evaluation.evaluator]: [0mInference done 70/72. Dataloading: 0.0990 s/iter. Inference: 11.6195 s/iter. Eval: 5.2428 s/iter. Total: 16.9616 s/iter. ETA=0:00:33
43
34
36
25
51
[32m[06/13 11:43:53 d2.evaluation.evaluator]: [0mInference done 71/72. Dataloading: 0.0992 s/iter. Inference: 11.6306 s/iter. Eval: 5.2498 s/iter. Total: 16.9800 s/iter. ETA=0:00:16
43
34
72
51
25
37
97
28
87
[32m[06/13 11:43:55 d2.evaluation.evaluator]: [0mTotal inference time: 0:18:43.124954 (16.763059 s / iter per device, on 1 devices)
[32m[06/13 11:43:55 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:12:49 (11.481451 s / iter per device, on 1 devices)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [29:00<00:00, 24.18s/it]
==> 15-frame vpq_stat: 1741.7963860034943 sec
{'test_VPQ/__background___PQ': 0.0, 'test_Prec/__background__': 0.0, 'test_Rec/__background__': 0.0, 'test_VPQ/instrument_table_PQ': 0.0, 'test_Prec/instrument_table': 0, 'test_Rec/instrument_table': 0.0, 'test_VPQ/ae_PQ': 0.0, 'test_Prec/ae': 0.0, 'test_Rec/ae': 0.0, 'test_VPQ/ot_PQ': 0.0, 'test_Prec/ot': 0, 'test_Rec/ot': 0.0, 'test_VPQ/mps_station_PQ': 0.0, 'test_Prec/mps_station': 0, 'test_Rec/mps_station': 0.0, 'test_VPQ/patient_PQ': 0.0, 'test_Prec/patient': 0, 'test_Rec/patient': 0.0, 'test_VPQ/drape_PQ': 0.0, 'test_Prec/drape': 0, 'test_Rec/drape': 0.0, 'test_VPQ/anest_PQ': 0.0, 'test_Prec/anest': 0, 'test_Rec/anest': 0.0, 'test_VPQ/circulator_PQ': 0.0, 'test_Prec/circulator': 0, 'test_Rec/circulator': 0.0, 'test_VPQ/assistant_surgeon_PQ': 0.0, 'test_Prec/assistant_surgeon': 0, 'test_Rec/assistant_surgeon': 0.0, 'test_VPQ/head_surgeon_PQ': 0.0, 'test_Prec/head_surgeon': 0, 'test_Rec/head_surgeon': 0.0, 'test_VPQ/mps_PQ': 0.0, 'test_Prec/mps': 0, 'test_Rec/mps': 0.0, 'test_VPQ/nurse_PQ': 0.0, 'test_Prec/nurse': 0, 'test_Rec/nurse': 0.0, 'test_VPQ/drill_PQ': 0.0, 'test_Prec/drill': 0, 'test_Rec/drill': 0.0, 'test_VPQ/hammer_PQ': 0.0, 'test_Prec/hammer': 0, 'test_Rec/hammer': 0.0, 'test_VPQ/saw_PQ': 0.0, 'test_Prec/saw': 0, 'test_Rec/saw': 0.0, 'test_VPQ/tracker_PQ': 0.0, 'test_Prec/tracker': 0, 'test_Rec/tracker': 0.0, 'test_VPQ/mako_robot_PQ': 0.0024091943117390303, 'test_Prec/mako_robot': 0.003339651593104071, 'test_Rec/mako_robot': 0.0031659108411055017, 'test_VPQ/monitor_PQ': 0.0, 'test_Prec/monitor': 0, 'test_Rec/monitor': 0.0, 'test_VPQ/c_arm_PQ': 0.0, 'test_Prec/c_arm': 0, 'test_Rec/c_arm': 0.0, 'test_VPQ/unrelated_person_PQ': 0.0, 'test_Prec/unrelated_person': 0, 'test_Rec/unrelated_person': 0.0, 'test_VPQ/student_PQ': 0.0, 'test_Prec/student': 0.0, 'test_Rec/student': 0.0, 'test_VPQ/secondary_table_PQ': 0.0, 'test_Prec/secondary_table': 0.0, 'test_Rec/secondary_table': 0.0, 'test_VPQ/cementer_PQ': 0.0, 'test_Prec/cementer': 0.0, 'test_Rec/cementer': 0.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [58:00<00:00, 48.34s/it]
==> 35-frame vpq_stat: 3480.9572143554688 sec
{'test_VPQ/__background___PQ': 0.0, 'test_Prec/__background__': 0.0, 'test_Rec/__background__': 0.0, 'test_VPQ/instrument_table_PQ': 0.0, 'test_Prec/instrument_table': 0, 'test_Rec/instrument_table': 0.0, 'test_VPQ/ae_PQ': 0.0, 'test_Prec/ae': 0.0, 'test_Rec/ae': 0.0, 'test_VPQ/ot_PQ': 0.0, 'test_Prec/ot': 0, 'test_Rec/ot': 0.0, 'test_VPQ/mps_station_PQ': 0.0, 'test_Prec/mps_station': 0, 'test_Rec/mps_station': 0.0, 'test_VPQ/patient_PQ': 0.0, 'test_Prec/patient': 0, 'test_Rec/patient': 0.0, 'test_VPQ/drape_PQ': 0.0, 'test_Prec/drape': 0, 'test_Rec/drape': 0.0, 'test_VPQ/anest_PQ': 0.0, 'test_Prec/anest': 0, 'test_Rec/anest': 0.0, 'test_VPQ/circulator_PQ': 0.0, 'test_Prec/circulator': 0, 'test_Rec/circulator': 0.0, 'test_VPQ/assistant_surgeon_PQ': 0.0, 'test_Prec/assistant_surgeon': 0, 'test_Rec/assistant_surgeon': 0.0, 'test_VPQ/head_surgeon_PQ': 0.0, 'test_Prec/head_surgeon': 0, 'test_Rec/head_surgeon': 0.0, 'test_VPQ/mps_PQ': 0.0, 'test_Prec/mps': 0, 'test_Rec/mps': 0.0, 'test_VPQ/nurse_PQ': 0.0, 'test_Prec/nurse': 0, 'test_Rec/nurse': 0.0, 'test_VPQ/drill_PQ': 0.0, 'test_Prec/drill': 0, 'test_Rec/drill': 0.0, 'test_VPQ/hammer_PQ': 0.0, 'test_Prec/hammer': 0, 'test_Rec/hammer': 0.0, 'test_VPQ/saw_PQ': 0.0, 'test_Prec/saw': 0, 'test_Rec/saw': 0.0, 'test_VPQ/tracker_PQ': 0.0, 'test_Prec/tracker': 0, 'test_Rec/tracker': 0.0, 'test_VPQ/mako_robot_PQ': 0.0013825169714546358, 'test_Prec/mako_robot': 0.0019185090443997808, 'test_Rec/mako_robot': 0.0018313421121479026, 'test_VPQ/monitor_PQ': 0.0, 'test_Prec/monitor': 0, 'test_Rec/monitor': 0.0, 'test_VPQ/c_arm_PQ': 0.0, 'test_Prec/c_arm': 0, 'test_Rec/c_arm': 0.0, 'test_VPQ/unrelated_person_PQ': 0.0, 'test_Prec/unrelated_person': 0, 'test_Rec/unrelated_person': 0.0, 'test_VPQ/student_PQ': 0.0, 'test_Prec/student': 0.0, 'test_Rec/student': 0.0, 'test_VPQ/secondary_table_PQ': 0.0, 'test_Prec/secondary_table': 0.0, 'test_Rec/secondary_table': 0.0, 'test_VPQ/cementer_PQ': 0.0, 'test_Prec/cementer': 0.0, 'test_Rec/cementer': 0.0}
[32m[06/13 13:11:01 d2.evaluation.testing]: [0mcopypaste: test_vpq_all=0.009479278207984165
[32m[06/13 13:11:01 d2.evaluation.testing]: [0mcopypaste: test_prec=0.00013145401593759627
[32m[06/13 13:11:01 d2.evaluation.testing]: [0mcopypaste: test_rec=0.00012493132383133512
