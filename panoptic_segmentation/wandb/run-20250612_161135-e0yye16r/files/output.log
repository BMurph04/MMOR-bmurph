Frequency file exists. Using weights from frequency file
Using no loss weighting...
[32m[06/12 16:11:37 d2.engine.defaults]: [0mModel:
CTMinVIS(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): VideoMultiScaleMaskedTransformerDecoder_dvisPlus(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=125, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (reid_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion VideoSetCriterion
      matcher: Matcher VideoHungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 124
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (image_matcher): Matcher HungarianMatcher
      cost_class: 2.0
      cost_mask: 5.0
      cost_dice: 5.0
  (cl_plugin): CTCLPlugin()
)
[32m[06/12 16:11:37 fvcore.common.checkpoint]: [0m[Checkpointer] Loading from checkpoints/ctvis_r50_vspw.pth ...
Loading samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:01<00:00, 37.47it/s]
[32m[06/12 16:11:38 d2.data.common]: [0mSerializing 72 elements to byte tensors and concatenating them all ...
[32m[06/12 16:11:38 d2.data.common]: [0mSerialized dataset takes 14.05 MiB
COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.
[32m[06/12 16:11:38 d2.evaluation.evaluator]: [0mStart inference on 72 batches
/home/connecteve/miniconda3/envs/mm-or/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
43
34
51
89
91
37
85
[32m[06/12 16:12:04 d2.evaluation.evaluator]: [0mInference done 1/72. Dataloading: 0.1733 s/iter. Inference: 18.8357 s/iter. Eval: 6.3280 s/iter. Total: 25.3376 s/iter. ETA=0:29:58
43
34
51
91
47
28
28
[32m[06/12 16:12:27 d2.evaluation.evaluator]: [0mInference done 2/72. Dataloading: 0.1225 s/iter. Inference: 18.0985 s/iter. Eval: 6.3261 s/iter. Total: 24.5477 s/iter. ETA=0:28:38
43
34
51
85
[32m[06/12 16:12:51 d2.evaluation.evaluator]: [0mInference done 3/72. Dataloading: 0.1261 s/iter. Inference: 17.8166 s/iter. Eval: 6.4254 s/iter. Total: 24.3685 s/iter. ETA=0:28:01
43
34
51
47
91
25
[32m[06/12 16:13:16 d2.evaluation.evaluator]: [0mInference done 4/72. Dataloading: 0.0947 s/iter. Inference: 17.7750 s/iter. Eval: 6.4683 s/iter. Total: 24.3384 s/iter. ETA=0:27:35
43
51
34
28
[32m[06/12 16:13:34 d2.evaluation.evaluator]: [0mInference done 5/72. Dataloading: 0.0759 s/iter. Inference: 17.6726 s/iter. Eval: 5.4414 s/iter. Total: 23.1903 s/iter. ETA=0:25:53
43
34
37
51
36
36
31
36
36
32
36
84
37
87
43
34
28
47
47
97
[32m[06/12 16:13:54 d2.evaluation.evaluator]: [0mInference done 7/72. Dataloading: 0.0001 s/iter. Inference: 9.2522 s/iter. Eval: 0.7754 s/iter. Total: 10.0277 s/iter. ETA=0:10:51
43
34
91
28
97
[32m[06/12 16:14:18 d2.evaluation.evaluator]: [0mInference done 8/72. Dataloading: 0.0210 s/iter. Inference: 11.7772 s/iter. Eval: 2.6649 s/iter. Total: 14.4632 s/iter. ETA=0:15:25
43
34
47
28
[32m[06/12 16:14:41 d2.evaluation.evaluator]: [0mInference done 9/72. Dataloading: 0.0158 s/iter. Inference: 13.1756 s/iter. Eval: 3.6188 s/iter. Total: 16.8105 s/iter. ETA=0:17:39
43
34
91
28
47
[32m[06/12 16:15:05 d2.evaluation.evaluator]: [0mInference done 10/72. Dataloading: 0.0371 s/iter. Inference: 13.8607 s/iter. Eval: 4.1784 s/iter. Total: 18.0765 s/iter. ETA=0:18:40
34
43
91
47
28
[32m[06/12 16:15:28 d2.evaluation.evaluator]: [0mInference done 11/72. Dataloading: 0.0487 s/iter. Inference: 14.3167 s/iter. Eval: 4.5517 s/iter. Total: 18.9174 s/iter. ETA=0:19:13
43
34
37
72
97
32
71
37
47
43
34
51
37
91
85
[32m[06/12 16:15:53 d2.evaluation.evaluator]: [0mInference done 13/72. Dataloading: 0.0366 s/iter. Inference: 12.9971 s/iter. Eval: 4.2751 s/iter. Total: 17.3091 s/iter. ETA=0:17:01
43
34
85
25
[32m[06/12 16:16:16 d2.evaluation.evaluator]: [0mInference done 14/72. Dataloading: 0.0326 s/iter. Inference: 13.4212 s/iter. Eval: 4.5059 s/iter. Total: 17.9600 s/iter. ETA=0:17:21
34
43
25
51
[32m[06/12 16:16:39 d2.evaluation.evaluator]: [0mInference done 15/72. Dataloading: 0.0295 s/iter. Inference: 13.7304 s/iter. Eval: 4.6945 s/iter. Total: 18.4548 s/iter. ETA=0:17:31
34
43
72
25
[32m[06/12 16:17:02 d2.evaluation.evaluator]: [0mInference done 16/72. Dataloading: 0.0345 s/iter. Inference: 14.0040 s/iter. Eval: 4.8456 s/iter. Total: 18.8844 s/iter. ETA=0:17:37
43
34
51
28
[32m[06/12 16:17:25 d2.evaluation.evaluator]: [0mInference done 17/72. Dataloading: 0.0317 s/iter. Inference: 14.2241 s/iter. Eval: 4.9713 s/iter. Total: 19.2275 s/iter. ETA=0:17:37
34
43
37
51
25
72
25
87
25
35
97
31
36
43
34
47
72
[32m[06/12 16:17:50 d2.evaluation.evaluator]: [0mInference done 19/72. Dataloading: 0.0394 s/iter. Inference: 13.5302 s/iter. Eval: 4.7159 s/iter. Total: 18.2859 s/iter. ETA=0:16:09
43
34
47
36
48
28
37
[32m[06/12 16:18:13 d2.evaluation.evaluator]: [0mInference done 20/72. Dataloading: 0.0368 s/iter. Inference: 13.7849 s/iter. Eval: 4.7955 s/iter. Total: 18.6176 s/iter. ETA=0:16:08
43
34
91
35
[32m[06/12 16:18:36 d2.evaluation.evaluator]: [0mInference done 21/72. Dataloading: 0.0347 s/iter. Inference: 13.9501 s/iter. Eval: 4.8698 s/iter. Total: 18.8550 s/iter. ETA=0:16:01
43
34
97
36
28
28
35
[32m[06/12 16:18:58 d2.evaluation.evaluator]: [0mInference done 22/72. Dataloading: 0.0327 s/iter. Inference: 14.0965 s/iter. Eval: 4.9489 s/iter. Total: 19.0785 s/iter. ETA=0:15:53
43
34
35
47
84
28
28
[32m[06/12 16:19:21 d2.evaluation.evaluator]: [0mInference done 23/72. Dataloading: 0.0309 s/iter. Inference: 14.2321 s/iter. Eval: 5.0133 s/iter. Total: 19.2768 s/iter. ETA=0:15:44
43
34
36
35
97
[32m[06/12 16:19:44 d2.evaluation.evaluator]: [0mInference done 24/72. Dataloading: 0.0363 s/iter. Inference: 14.3513 s/iter. Eval: 5.0937 s/iter. Total: 19.4817 s/iter. ETA=0:15:35
43
34
25
84
28
51
91
48
25
28
[32m[06/12 16:20:07 d2.evaluation.evaluator]: [0mInference done 25/72. Dataloading: 0.0345 s/iter. Inference: 14.4502 s/iter. Eval: 5.1593 s/iter. Total: 19.6443 s/iter. ETA=0:15:23
43
34
28
[32m[06/12 16:20:30 d2.evaluation.evaluator]: [0mInference done 26/72. Dataloading: 0.0340 s/iter. Inference: 14.5650 s/iter. Eval: 5.2080 s/iter. Total: 19.8075 s/iter. ETA=0:15:11
43
34
52
28
35
[32m[06/12 16:20:53 d2.evaluation.evaluator]: [0mInference done 27/72. Dataloading: 0.0386 s/iter. Inference: 14.6652 s/iter. Eval: 5.2455 s/iter. Total: 19.9497 s/iter. ETA=0:14:57
43
34
28
28
28
51
[32m[06/12 16:21:16 d2.evaluation.evaluator]: [0mInference done 28/72. Dataloading: 0.0369 s/iter. Inference: 14.7635 s/iter. Eval: 5.2839 s/iter. Total: 20.0848 s/iter. ETA=0:14:43
43
34
28
28
[32m[06/12 16:21:39 d2.evaluation.evaluator]: [0mInference done 29/72. Dataloading: 0.0355 s/iter. Inference: 14.8602 s/iter. Eval: 5.3244 s/iter. Total: 20.2206 s/iter. ETA=0:14:29
43
34
51
36
55
25
[32m[06/12 16:22:02 d2.evaluation.evaluator]: [0mInference done 30/72. Dataloading: 0.0342 s/iter. Inference: 14.9269 s/iter. Eval: 5.3477 s/iter. Total: 20.3093 s/iter. ETA=0:14:12
43
34
47
51
51
28
[32m[06/12 16:22:16 d2.evaluation.evaluator]: [0mInference done 31/72. Dataloading: 0.0329 s/iter. Inference: 14.7557 s/iter. Eval: 5.2803 s/iter. Total: 20.0694 s/iter. ETA=0:13:42
43
34
25
25
51
51
25
28
36
[32m[06/12 16:22:39 d2.evaluation.evaluator]: [0mInference done 32/72. Dataloading: 0.0352 s/iter. Inference: 14.8457 s/iter. Eval: 5.3055 s/iter. Total: 20.1869 s/iter. ETA=0:13:27
43
34
28
28
25
28
36
28
[32m[06/12 16:23:02 d2.evaluation.evaluator]: [0mInference done 33/72. Dataloading: 0.0339 s/iter. Inference: 14.9263 s/iter. Eval: 5.3350 s/iter. Total: 20.2957 s/iter. ETA=0:13:11
43
34
51
28
36
25
28
28
[32m[06/12 16:23:25 d2.evaluation.evaluator]: [0mInference done 34/72. Dataloading: 0.0365 s/iter. Inference: 14.9822 s/iter. Eval: 5.3632 s/iter. Total: 20.3824 s/iter. ETA=0:12:54
43
34
51
91
36
28
36
25
51
[32m[06/12 16:23:49 d2.evaluation.evaluator]: [0mInference done 35/72. Dataloading: 0.0483 s/iter. Inference: 15.0506 s/iter. Eval: 5.4021 s/iter. Total: 20.5015 s/iter. ETA=0:12:38
43
34
28
93
36
25
51
51
[32m[06/12 16:24:12 d2.evaluation.evaluator]: [0mInference done 36/72. Dataloading: 0.0468 s/iter. Inference: 15.0972 s/iter. Eval: 5.4324 s/iter. Total: 20.5769 s/iter. ETA=0:12:20
43
34
28
36
25
25
25
[32m[06/12 16:24:35 d2.evaluation.evaluator]: [0mInference done 37/72. Dataloading: 0.0453 s/iter. Inference: 15.1454 s/iter. Eval: 5.4655 s/iter. Total: 20.6568 s/iter. ETA=0:12:02
43
34
51
25
25
36
28
[32m[06/12 16:24:58 d2.evaluation.evaluator]: [0mInference done 38/72. Dataloading: 0.0440 s/iter. Inference: 15.1871 s/iter. Eval: 5.4956 s/iter. Total: 20.7271 s/iter. ETA=0:11:44
43
34
51
36
25
28
25
25
36
[32m[06/12 16:25:21 d2.evaluation.evaluator]: [0mInference done 39/72. Dataloading: 0.0427 s/iter. Inference: 15.2254 s/iter. Eval: 5.5226 s/iter. Total: 20.7913 s/iter. ETA=0:11:26
43
34
51
36
25
36
36
25
[32m[06/12 16:25:44 d2.evaluation.evaluator]: [0mInference done 40/72. Dataloading: 0.0457 s/iter. Inference: 15.2646 s/iter. Eval: 5.5488 s/iter. Total: 20.8596 s/iter. ETA=0:11:07
43
34
51
91
25
[32m[06/12 16:26:07 d2.evaluation.evaluator]: [0mInference done 41/72. Dataloading: 0.0444 s/iter. Inference: 15.3112 s/iter. Eval: 5.5696 s/iter. Total: 20.9257 s/iter. ETA=0:10:48
43
34
91
36
25
25
92
25
28
25
[32m[06/12 16:26:25 d2.evaluation.evaluator]: [0mInference done 42/72. Dataloading: 0.0474 s/iter. Inference: 15.2487 s/iter. Eval: 5.5463 s/iter. Total: 20.8430 s/iter. ETA=0:10:25
43
34
28
47
47
47
91
97
[32m[06/12 16:26:49 d2.evaluation.evaluator]: [0mInference done 43/72. Dataloading: 0.0482 s/iter. Inference: 15.2961 s/iter. Eval: 5.5705 s/iter. Total: 20.9154 s/iter. ETA=0:10:06
34
43
28
48
25
[32m[06/12 16:27:12 d2.evaluation.evaluator]: [0mInference done 44/72. Dataloading: 0.0470 s/iter. Inference: 15.3364 s/iter. Eval: 5.5917 s/iter. Total: 20.9756 s/iter. ETA=0:09:47
43
34
47
28
28
[32m[06/12 16:27:35 d2.evaluation.evaluator]: [0mInference done 45/72. Dataloading: 0.0458 s/iter. Inference: 15.3564 s/iter. Eval: 5.6092 s/iter. Total: 21.0119 s/iter. ETA=0:09:27
43
34
28
48
47
28
91
[32m[06/12 16:27:46 d2.evaluation.evaluator]: [0mInference done 46/72. Dataloading: 0.0447 s/iter. Inference: 15.1792 s/iter. Eval: 5.5446 s/iter. Total: 20.7690 s/iter. ETA=0:08:59
43
34
37
91
28
28
28
[32m[06/12 16:28:09 d2.evaluation.evaluator]: [0mInference done 47/72. Dataloading: 0.0437 s/iter. Inference: 15.2203 s/iter. Eval: 5.5542 s/iter. Total: 20.8187 s/iter. ETA=0:08:40
43
34
91
[32m[06/12 16:28:32 d2.evaluation.evaluator]: [0mInference done 48/72. Dataloading: 0.0440 s/iter. Inference: 15.2581 s/iter. Eval: 5.5690 s/iter. Total: 20.8717 s/iter. ETA=0:08:20
34
43
91
25
[32m[06/12 16:28:54 d2.evaluation.evaluator]: [0mInference done 49/72. Dataloading: 0.0430 s/iter. Inference: 15.2878 s/iter. Eval: 5.5783 s/iter. Total: 20.9097 s/iter. ETA=0:08:00
43
34
28
91
37
[32m[06/12 16:29:06 d2.evaluation.evaluator]: [0mInference done 50/72. Dataloading: 0.0429 s/iter. Inference: 15.1403 s/iter. Eval: 5.5196 s/iter. Total: 20.7033 s/iter. ETA=0:07:35
43
34
25
28
28
25
[32m[06/12 16:29:29 d2.evaluation.evaluator]: [0mInference done 51/72. Dataloading: 0.0431 s/iter. Inference: 15.1808 s/iter. Eval: 5.5295 s/iter. Total: 20.7540 s/iter. ETA=0:07:15
43
34
36
84
25
25
97
[32m[06/12 16:29:51 d2.evaluation.evaluator]: [0mInference done 52/72. Dataloading: 0.0422 s/iter. Inference: 15.2077 s/iter. Eval: 5.5355 s/iter. Total: 20.7859 s/iter. ETA=0:06:55
43
34
36
36
84
25
28
[32m[06/12 16:30:14 d2.evaluation.evaluator]: [0mInference done 53/72. Dataloading: 0.0413 s/iter. Inference: 15.2372 s/iter. Eval: 5.5420 s/iter. Total: 20.8211 s/iter. ETA=0:06:35
43
34
36
25
28
91
97
54
[32m[06/12 16:30:25 d2.evaluation.evaluator]: [0mInference done 54/72. Dataloading: 0.0405 s/iter. Inference: 15.1074 s/iter. Eval: 5.4875 s/iter. Total: 20.6360 s/iter. ETA=0:06:11
43
34
36
47
25
97
[32m[06/12 16:30:48 d2.evaluation.evaluator]: [0mInference done 55/72. Dataloading: 0.0397 s/iter. Inference: 15.1374 s/iter. Eval: 5.4983 s/iter. Total: 20.6759 s/iter. ETA=0:05:51
43
34
36
49
36
28
25
[32m[06/12 16:31:11 d2.evaluation.evaluator]: [0mInference done 56/72. Dataloading: 0.0399 s/iter. Inference: 15.1642 s/iter. Eval: 5.5141 s/iter. Total: 20.7188 s/iter. ETA=0:05:31
43
34
47
28
47
28
27
52
[32m[06/12 16:31:34 d2.evaluation.evaluator]: [0mInference done 57/72. Dataloading: 0.0391 s/iter. Inference: 15.1930 s/iter. Eval: 5.5268 s/iter. Total: 20.7595 s/iter. ETA=0:05:11
43
34
47
52
51
28
[32m[06/12 16:31:56 d2.evaluation.evaluator]: [0mInference done 58/72. Dataloading: 0.0403 s/iter. Inference: 15.2137 s/iter. Eval: 5.5415 s/iter. Total: 20.7961 s/iter. ETA=0:04:51
34
43
28
28
28
25
[32m[06/12 16:32:19 d2.evaluation.evaluator]: [0mInference done 59/72. Dataloading: 0.0424 s/iter. Inference: 15.2357 s/iter. Eval: 5.5508 s/iter. Total: 20.8294 s/iter. ETA=0:04:30
43
34
25
37
51
84
51
[32m[06/12 16:32:41 d2.evaluation.evaluator]: [0mInference done 60/72. Dataloading: 0.0416 s/iter. Inference: 15.2558 s/iter. Eval: 5.5598 s/iter. Total: 20.8577 s/iter. ETA=0:04:10
43
34
89
28
91
37
[32m[06/12 16:33:04 d2.evaluation.evaluator]: [0mInference done 61/72. Dataloading: 0.0409 s/iter. Inference: 15.2753 s/iter. Eval: 5.5728 s/iter. Total: 20.8896 s/iter. ETA=0:03:49
34
43
91
28
28
[32m[06/12 16:33:27 d2.evaluation.evaluator]: [0mInference done 62/72. Dataloading: 0.0402 s/iter. Inference: 15.3059 s/iter. Eval: 5.5816 s/iter. Total: 20.9282 s/iter. ETA=0:03:29
34
43
28
28
91
[32m[06/12 16:33:50 d2.evaluation.evaluator]: [0mInference done 63/72. Dataloading: 0.0395 s/iter. Inference: 15.3244 s/iter. Eval: 5.5905 s/iter. Total: 20.9550 s/iter. ETA=0:03:08
43
34
37
91
28
[32m[06/12 16:34:12 d2.evaluation.evaluator]: [0mInference done 64/72. Dataloading: 0.0398 s/iter. Inference: 15.3429 s/iter. Eval: 5.5984 s/iter. Total: 20.9817 s/iter. ETA=0:02:47
34
43
[32m[06/12 16:34:35 d2.evaluation.evaluator]: [0mInference done 65/72. Dataloading: 0.0392 s/iter. Inference: 15.3696 s/iter. Eval: 5.6057 s/iter. Total: 21.0149 s/iter. ETA=0:02:27
34
43
91
36
28
28
28
28
[32m[06/12 16:34:49 d2.evaluation.evaluator]: [0mInference done 66/72. Dataloading: 0.0393 s/iter. Inference: 15.2859 s/iter. Eval: 5.5753 s/iter. Total: 20.9010 s/iter. ETA=0:02:05
43
34
37
[32m[06/12 16:35:12 d2.evaluation.evaluator]: [0mInference done 67/72. Dataloading: 0.0399 s/iter. Inference: 15.3096 s/iter. Eval: 5.5873 s/iter. Total: 20.9373 s/iter. ETA=0:01:44
43
34
84
91
49
[32m[06/12 16:35:35 d2.evaluation.evaluator]: [0mInference done 68/72. Dataloading: 0.0393 s/iter. Inference: 15.3280 s/iter. Eval: 5.6009 s/iter. Total: 20.9688 s/iter. ETA=0:01:23
43
34
36
84
51
[32m[06/12 16:35:58 d2.evaluation.evaluator]: [0mInference done 69/72. Dataloading: 0.0387 s/iter. Inference: 15.3508 s/iter. Eval: 5.6103 s/iter. Total: 21.0003 s/iter. ETA=0:01:03
43
34
36
84
27
28
[32m[06/12 16:36:21 d2.evaluation.evaluator]: [0mInference done 70/72. Dataloading: 0.0381 s/iter. Inference: 15.3726 s/iter. Eval: 5.6206 s/iter. Total: 21.0319 s/iter. ETA=0:00:42
43
34
36
51
25
51
[32m[06/12 16:36:44 d2.evaluation.evaluator]: [0mInference done 71/72. Dataloading: 0.0376 s/iter. Inference: 15.3865 s/iter. Eval: 5.6271 s/iter. Total: 21.0517 s/iter. ETA=0:00:21
43
34
84
37
72
37
51
87
47
27
97
[32m[06/12 16:36:47 d2.evaluation.evaluator]: [0mTotal inference time: 0:23:12.636689 (20.785622 s / iter per device, on 1 devices)
[32m[06/12 16:36:47 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:16:57 (15.190749 s / iter per device, on 1 devices)
 56%|████████████████████████████████████████████████████████████████████████████                                                             | 40/72 [23:03<14:45, 27.69s/it]
